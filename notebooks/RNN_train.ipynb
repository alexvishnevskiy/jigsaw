{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alexander/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexander/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/alexander/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/alexander/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jigsaw.models.linear_models.base_model import LinearModel, KernelModel, SVRModel\n",
    "from jigsaw.models.cnn_models.base_model import CnnModel\n",
    "from jigsaw.models.rnn_models.base_model import RnnModel\n",
    "from transformers import AutoTokenizer\n",
    "from jigsaw.utils.tokenizer import Tokenizer\n",
    "import optuna\n",
    "from box import Box\n",
    "from jigsaw.utils.glove import load_glove\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from jigsaw.scripts.training import rnn_train\n",
    "from jigsaw.scripts.inference import linear_predict\n",
    "from jigsaw.utils.cleaning import *\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'seed': 42,\n",
    "    'logger': {\n",
    "        'save_dir': 'models',\n",
    "        'project': 'Jigsaw',\n",
    "        'log_model': True\n",
    "    },\n",
    "    'dataset': {\n",
    "        'type': 'regression', #paired, regression\n",
    "        'name': 'toxic-comment-preprocessed',\n",
    "        'text_col': 'comment_text',\n",
    "        'target_col': 'y'\n",
    "    },\n",
    "    'model_name': 'roberta-base',\n",
    "    'load_embeddings': False,\n",
    "    'freeze_embeddings': False,\n",
    "    'rnn_type': 'lstm', #only lstm, gru doesn't work for some reasons\n",
    "    'emb_size': 100,\n",
    "    'hidden_size': 100,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'max_length': 256,\n",
    "    'bucket_seq': True,\n",
    "    'rnn_embeddings': False,\n",
    "    'margin': 0.5,\n",
    "    'batch_size': 16,\n",
    "    'acc_step': 1,\n",
    "    'epoch': 5,\n",
    "    'num_classes': 1,\n",
    "    'optimizer': {\n",
    "        'name': 'optim.AdamW',\n",
    "        'params': {\n",
    "            'lr': 1e-2,\n",
    "            'weight_decay': 1e-5\n",
    "        }\n",
    "    },\n",
    "    'scheduler': {\n",
    "        'name': 'get_cosine_schedule_with_warmup',\n",
    "        'params': {\n",
    "            'num_warmup_steps': 0.06\n",
    "        }\n",
    "    },\n",
    "    'trainer': {\n",
    "        'progress_bar_refresh_rate': 3,\n",
    "        'num_sanity_val_steps': 2\n",
    "    }\n",
    "}\n",
    "  \n",
    "cfg = Box(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/preprocessed_data/jigsaw-toxic-comment_lemmatize.csv')\n",
    "val_df = pd.read_csv('../data/preprocessed_data/validation_data_lemmatize.csv')\n",
    "test_df = pd.read_csv('../data/jigsaw-rate-severity/comments_to_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne = train_df[train_df['y'] != 0].sample(10_000)\n",
    "ye = train_df[train_df['y'] == 0].sample(len(ne))\n",
    "train_df = pd.concat([ne, ye])\n",
    "train_df = train_df.sample(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    emb_type = trial.suggest_categorical('emb_type', ['glove', 'fasttext'])\n",
    "    hidden_size = trial.suggest_categorical('emb_size', [100, 200, 300])\n",
    "    tokenizer = 'pretrained'\n",
    "    \n",
    "    cfg.emb_type = emb_type\n",
    "    cfg.load_embeddings = True\n",
    "    cfg.freeze_embeddings = True\n",
    "    cfg.hidden_size = hidden_size\n",
    "    cfg.num_layers = 2\n",
    "    cfg.bidirectional = True\n",
    "    if tokenizer == 'own':\n",
    "        t = Tokenizer()\n",
    "        t.fit(train_df)\n",
    "        cfg['tokenizer'] = t\n",
    "    else:\n",
    "        cfg['tokenizer'] = AutoTokenizer.from_pretrained(cfg['model_name'])\n",
    "    \n",
    "    if emb_type == 'glove':\n",
    "        emb_path_dict = {\n",
    "            'twitter_27B_25d': '/Users/alexander/Documents/jigsaw/vectors/glove/glove.twitter.27B.25d.txt',\n",
    "            'twitter_27B_50d': '/Users/alexander/Documents/jigsaw/vectors/glove/glove.twitter.27B.50d.txt',\n",
    "            'twitter_27B_100d': '/Users/alexander/Documents/jigsaw/vectors/glove/glove.twitter.27B.100d.txt',\n",
    "            'twitter_27B_200d': '/Users/alexander/Documents/jigsaw/vectors/glove/glove.twitter.27B.200d.txt',\n",
    "            '840B_300d': '/Users/alexander/Documents/jigsaw/vectors/glove/glove.840B.300d.txt'\n",
    "        }\n",
    "        emb_path_key = trial.suggest_categorical('emb_path_glove', list(emb_path_dict.keys()))\n",
    "        emb_path = emb_path_dict[emb_path_key]\n",
    "        cfg.emb_path = emb_path\n",
    "        \n",
    "    if emb_type == 'fasttext':\n",
    "        emb_path_dict = {\n",
    "            'model_100_5': '/Users/alexander/Documents/jigsaw/vectors/fast_text/model_100_5.bin',\n",
    "            'model_200_5': '/Users/alexander/Documents/jigsaw/vectors/fast_text/model_200_5.bin',\n",
    "            'model_300_5': '/Users/alexander/Documents/jigsaw/vectors/fast_text/model_300_5.bin'\n",
    "        }\n",
    "        emb_path_key = trial.suggest_categorical('emb_path_fasttext', list(emb_path_dict.keys()))\n",
    "        emb_path = emb_path_dict[emb_path_key]\n",
    "        cfg.emb_path = emb_path\n",
    "    \n",
    "    if tokenizer == 'own':\n",
    "        acc = rnn_train(cfg, train_df, val_df, checkpoint_args=['optuna'], limit_train_batches = 1245)\n",
    "    else:\n",
    "        acc = rnn_train(cfg, train_df, val_df, checkpoint_args=['optuna'])\n",
    "    return acc\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
